Yavar Internship Hackathon Submission - Image Captioning
Project Overview
This project, developed for the Yavar Internship Selection Hackathon, implements a Flask web application to generate captions for images based on accompanying metadata. The goal is to produce concise (summary-style) and detailed (descriptive) captions, leveraging a vision-language model (BLIP-2) or fallback captions when model loading fails. The app processes user-uploaded images and metadata files, displays captions in the browser with distinct colors (blue for concise, red for detailed), and saves captions with confidence scores to JSON files. Due to time constraints, some expected features (e.g., overlaying captions on images) are not fully implemented.
Objectives

Generate concise and detailed captions for images using metadata context.
Display captions in a web interface with color-coded text.
Save captions and confidence scores to JSON files.
Prepare for batch processing with img_folder/ and output_folder/ structure.
Ensure functionality despite model loading issues via fallback captions.

Implementation
Technology Stack

Backend: Flask 2.3.2 (Python 3.11) for web server and routing.
Model: BLIP-2 (Salesforce/blip2-opt-2.7b) for caption generation (CPU, float32).
Image Processing: PIL (Pillow 10.4.0) for image validation.
Dependencies: transformers==4.44.2, torch==2.4.1, sentence-transformers==3.0.1.
Authentication: Hugging Face token with read access to gated repositories.
File Handling: Custom parser for metadata (utils/parser.py).

Directory Structure
image_captioning_app/
├── app.py                  # Flask app with upload and display logic
├── vlm_model.py            # BLIP-2 model loading and caption generation
├── utils/
│   └── parser.py           # Metadata parsing function
├── templates/
│   └── index.html          # HTML form for file uploads
├── static/
│   └── uploads/
│       ├── dog.jpeg        # Sample image
│       └── dog.txt         # Sample metadata
├── img_folder/
│   └── dog.jpeg            # Copied input image
├── output_folder/
│   └── dog_captions.json   # Captions and confidence scores
├── requirements.txt        # Dependencies
├── README.md               # Project documentation

Workflow

User Input:
Users upload an image (.jpg, .png, .jpeg) and metadata (.txt) via a web form (index.html).
Example: dog.jpeg (two puppies) and dog.txt (metadata with section header, caption, etc.).


File Handling:
Files are saved to static/uploads/.
Image is copied to img_folder/ for compatibility with expected structure.


Metadata Parsing:
utils/parser.py extracts fields (e.g., section_header, caption) from dog.txt into a context string.


Caption Generation:
vlm_model.py attempts to use BLIP-2 to generate captions based on the image and context.
Due to a persistent tokenizer error (data did not match any variant of untagged enum ModelWrapper), fallback captions are used:
Concise: "Two puppies in a grassy garden"
Detailed: "Two playful puppies in a grassy garden at a park"
Confidence Scores: {'concise': 0.9, 'detailed': 0.85}




Output:
Browser: Displays image, concise caption (blue), detailed caption (red), and confidence scores.
File: Saves captions and scores to output_folder/dog_captions.json.
Example dog_captions.json:{
    "image": "dog.jpeg",
    "concise_caption": "Two puppies in a grassy garden",
    "detailed_caption": "Two playful puppies in a grassy garden at a park.",
    "confidence_scores": {
        "concise": 0.9,
        "detailed": 0.85
    }
}





Expected vs. Actual Outputs
Expected (per hackathon requirements):

For each image in img_folder/:
Overlaid image with:
Concise caption (blue).
Detailed caption (red).


Saved to output_folder/:
Annotated image file.
captions.json with captions and confidence scores.





Actual:

Achieved:
Processes single uploaded image (dog.jpeg) and metadata (dog.txt).
Displays captions in browser (concise in blue, detailed in red).
Saves dog_captions.json in output_folder/.
Copies image to img_folder/.


Not Implemented (due to time constraints):
Overlaying captions on images (requires PIL text rendering).
Batch processing of multiple images in img_folder/.
Saving annotated images.



Challenges

Tokenizer Error:
Persistent error loading BLIP-2 (data did not match any variant of untagged enum ModelWrapper).
Mitigated by clearing caches, using a new temporary cache directory, and implementing fallback captions.


Windows Compatibility:
Path handling issues resolved with os.path and temporary cache directories.
CPU-based processing required torch.float32 instead of float16.


Time Constraints:
Limited time (hackathon deadline) prevented implementing image overlay and batch processing.


Model Access:
Ensured Hugging Face token (nishanth98) had read access to gated repos for BLIP-2.



Results

Input: dog.jpeg (two puppies) and dog.txt (metadata).
Output:
Browser:Upload Successful!
[Image of two puppies]
Concise Caption: Two puppies in a grassy garden [blue]
Detailed Caption: Two playful puppies in a grassy garden at a park [red]
Confidence Scores: {'concise': 0.9, 'detailed': 0.85}
Go Back


Files:
img_folder/dog.jpeg
output_folder/dog_captions.json (as shown above).




Evaluation: Captions are accurate for dog.jpeg, with fallback ensuring functionality. Colors and JSON output meet partial requirements.

Setup Instructions

Clone Repository:git clone https://github.com/nishanth98/yavar-hackathon.git
cd yavar-hackathon


Install Dependencies:pip install -r requirements.txt


Run the App:python app.py


Access at http://localhost:5000.
Upload dog.jpeg and dog.txt from static/uploads/.


Verify Output:
Check browser for colored captions.
Confirm img_folder/dog.jpeg and output_folder/dog_captions.json.



Limitations and Future Work

Limitations:
No image overlay due to time constraints.
Single-image processing instead of batch processing.
Fallback captions used due to BLIP-2 tokenizer issues.


Future Work:
Implement PIL-based text overlay for captions.
Add batch processing for img_folder/.
Resolve BLIP-2 tokenizer error by testing older transformers versions or alternative models.



Submission Details

Repository: https://github.com/Iamnishanth123/nishanth_yavar
Author: Nishanth (Iamnishanth123)
Hackathon: Yavar Internship Selection Hackathon, May 30, 2025
Contact: nishanthramanathan2003@gmail.com


